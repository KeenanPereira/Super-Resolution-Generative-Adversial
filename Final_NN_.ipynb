{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9LNd2_fFqDi",
        "outputId": "6f00547a-aee9-4f7d-f73f-36b7baf05a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pillow scikit-image tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir('DIV2K_train_HR'):\n",
        "    !wget -q https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
        "    !unzip -q DIV2K_train_HR.zip\n",
        "    !rm DIV2K_train_HR.zip"
      ],
      "metadata": {
        "id": "z6cxfDsEFqz3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def create_lr(hr_dir='DIV2K_train_HR', lr_dir='DIV2K_train_LR', scale=4):\n",
        "    os.makedirs(lr_dir, exist_ok=True)\n",
        "    for fn in os.listdir(hr_dir):\n",
        "        if not fn.lower().endswith(('png','jpg','jpeg')): continue\n",
        "        hr = Image.open(f'{hr_dir}/{fn}').convert('RGB')\n",
        "        w,h = hr.size\n",
        "        lr = hr.resize((w//scale, h//scale), Image.BICUBIC).resize((w,h), Image.BICUBIC)\n",
        "        lr.save(f'{lr_dir}/{fn}')\n",
        "create_lr()"
      ],
      "metadata": {
        "id": "HFEGhnrHFq2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DIV2KDataset(Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
        "        self.hr_dir, self.lr_dir = hr_dir, lr_dir\n",
        "        self.fns = [f for f in os.listdir(hr_dir) if f.lower().endswith(('png','jpg'))]\n",
        "        self.transform = transform or Compose([ToTensor(), Normalize((0.5,)*3,(0.5,)*3)])\n",
        "    def __len__(self): return len(self.fns)\n",
        "    def __getitem__(self, i):\n",
        "        hr = Image.open(f'{self.hr_dir}/{self.fns[i]}').convert('RGB')\n",
        "        lr = Image.open(f'{self.lr_dir}/{self.fns[i]}').convert('RGB')\n",
        "        return self.transform(lr), self.transform(hr)"
      ],
      "metadata": {
        "id": "qM8HhwVfFq51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, c=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(c,c,3,1,1), nn.BatchNorm2d(c), nn.PReLU(),\n",
        "            nn.Conv2d(c,c,3,1,1), nn.BatchNorm2d(c)\n",
        "        )\n",
        "    def forward(self,x): return x + self.net(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self,c,scale=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(c, c*scale*scale, 3,1,1),\n",
        "            nn.PixelShuffle(scale),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_res=16, up=4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,64,9,1,4); self.pre1=nn.PReLU()\n",
        "        self.res  = nn.Sequential(*[ResidualBlock(64) for _ in range(num_res)])\n",
        "        self.conv2= nn.Conv2d(64,64,3,1,1); self.bn2=nn.BatchNorm2d(64)\n",
        "        ups=[]\n",
        "        for _ in range(up//2): ups.append(UpsampleBlock(64,2))\n",
        "        self.ups = nn.Sequential(*ups)\n",
        "        self.conv3= nn.Conv2d(64,3,9,1,4)\n",
        "    def forward(self,x):\n",
        "        out1=self.pre1(self.conv1(x))\n",
        "        out = self.res(out1)\n",
        "        out = self.bn2(self.conv2(out)) + out1\n",
        "        out = self.ups(out)\n",
        "        out = self.conv3(out)\n",
        "        return (torch.tanh(out)+1)/2\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        def b(in_c,out_c,s): layers.extend([nn.Conv2d(in_c,out_c,3,s,1), nn.BatchNorm2d(out_c), nn.LeakyReLU(0.2)])\n",
        "        b(3,64,1); b(64,64,2); b(64,128,1); b(128,128,2)\n",
        "        b(128,256,1); b(256,256,2); b(256,512,1); b(512,512,2)\n",
        "        layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(512,1024),\n",
        "                   nn.LeakyReLU(0.2), nn.Linear(1024,1)]\n",
        "        self.net=nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# ESRGAN RRDB\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    def __init__(self,in_c,gr=32):\n",
        "        super().__init__()\n",
        "        self.layers=nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.layers.append(nn.Sequential(\n",
        "                nn.Conv2d(in_c+i*gr, gr,3,1,1),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ))\n",
        "        self.conv1x1=nn.Conv2d(in_c+5*gr,in_c,1,1,0)\n",
        "    def forward(self,x):\n",
        "        feats=[x]\n",
        "        for layer in self.layers:\n",
        "            out=layer(torch.cat(feats,1)); feats.append(out)\n",
        "        out=torch.cat(feats,1)\n",
        "        return x + 0.2*self.conv1x1(out)\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self,in_c=64):\n",
        "        super().__init__()\n",
        "        self.rdb1=DenseResidualBlock(in_c)\n",
        "        self.rdb2=DenseResidualBlock(in_c)\n",
        "        self.rdb3=DenseResidualBlock(in_c)\n",
        "    def forward(self,x):\n",
        "        return x + 0.2*self.rdb3(self.rdb2(self.rdb1(x)))\n",
        "\n",
        "class EnhancedGenerator(Generator):\n",
        "    def __init__(self, rrdb_blocks=23):\n",
        "        super().__init__(num_res=0, up=4)\n",
        "        self.conv1 = nn.Conv2d(3,64,9,1,4); self.pre1=nn.PReLU()\n",
        "        self.trunk = nn.Sequential(*[RRDB(64) for _ in range(rrdb_blocks)])\n",
        "        self.trunk_conv = nn.Conv2d(64,64,3,1,1)\n",
        "        ups=[]\n",
        "        for _ in range(2):\n",
        "            ups += [nn.Conv2d(64,256,3,1,1), nn.PixelShuffle(2), nn.PReLU()]\n",
        "        self.ups = nn.Sequential(*ups)\n",
        "        self.conv_last = nn.Conv2d(64,3,9,1,4)\n",
        "    def forward(self,x):\n",
        "        out1=self.pre1(self.conv1(x))\n",
        "        out = self.trunk(out1)\n",
        "        out = self.trunk_conv(out) + out1\n",
        "        out = self.ups(out)\n",
        "        out = self.conv_last(out)\n",
        "        return (torch.tanh(out)+1)/2"
      ],
      "metadata": {
        "id": "lo1EebSiFq9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_srgan(hr_dir, lr_dir, epochs=200, bs=16, lr=1e-4, save_every=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    ds = DIV2KDataset(hr_dir, lr_dir)\n",
        "    dl = DataLoader(ds, bs, shuffle=True, num_workers=4)\n",
        "    G, D = Generator().to(device), Discriminator().to(device)\n",
        "    optG = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "    optD = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "    mse = nn.MSELoss(); bce=nn.BCEWithLogitsLoss()\n",
        "    for e in range(1, epochs+1):\n",
        "        G.train(); D.train()\n",
        "        for lr_imgs, hr_imgs in tqdm(dl, desc=f\"SRGAN Ep {e}/{epochs}\"):\n",
        "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "            valid = torch.ones(len(lr_imgs),1,device=device)\n",
        "            fake  = torch.zeros(len(lr_imgs),1,device=device)\n",
        "            # D step\n",
        "            gen_hr = G(lr_imgs).detach()\n",
        "            lossD = (bce(D(hr_imgs),valid)+bce(D(gen_hr),fake))/2\n",
        "            optD.zero_grad(); lossD.backward(); optD.step()\n",
        "            # G step\n",
        "            gen_hr = G(lr_imgs)\n",
        "            loss_content = mse(gen_hr, hr_imgs)\n",
        "            loss_adv = bce(D(gen_hr), valid)\n",
        "            loss_pix = mse(gen_hr, hr_imgs)\n",
        "            lossG = loss_content + 1e-3*loss_adv + 2e-6*loss_pix\n",
        "            optG.zero_grad(); lossG.backward(); optG.step()\n",
        "        if e % save_every==0:\n",
        "            torch.save(G.state_dict(),f'gen_{e}.pth')\n",
        "            torch.save(D.state_dict(),f'disc_{e}.pth')\n",
        "\n",
        "def train_enhanced(hr_dir, lr_dir, sr_ckpt=None, epochs=100, bs=16, lr=1e-4, save_every=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    ds = DIV2KDataset(hr_dir, lr_dir)\n",
        "    dl = DataLoader(ds, bs, shuffle=True, num_workers=4)\n",
        "    G = EnhancedGenerator().to(device)\n",
        "    if sr_ckpt: G.load_state_dict(torch.load(sr_ckpt))\n",
        "    D = Discriminator().to(device)\n",
        "    optG = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "    optD = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "    mse = nn.MSELoss(); bce=nn.BCEWithLogitsLoss()\n",
        "    for e in range(1, epochs+1):\n",
        "        G.train(); D.train()\n",
        "        for lr_imgs, hr_imgs in tqdm(dl, desc=f\"Enh Ep {e}/{epochs}\"):\n",
        "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "            valid = torch.ones(len(lr_imgs),1,device=device)\n",
        "            fake  = torch.zeros(len(lr_imgs),1,device=device)\n",
        "            # D step\n",
        "            gen_hr = G(lr_imgs).detach()\n",
        "            lossD = (bce(D(hr_imgs),valid)+bce(D(gen_hr),fake))/2\n",
        "            optD.zero_grad(); lossD.backward(); optD.step()\n",
        "            # G step\n",
        "            gen_hr = G(lr_imgs)\n",
        "            loss_content = mse(gen_hr, hr_imgs)\n",
        "            loss_adv = bce(D(gen_hr), valid)\n",
        "            loss_pix = mse(gen_hr, hr_imgs)\n",
        "            lossG = loss_content + 0.01*loss_adv + 0.006*loss_pix\n",
        "            optG.zero_grad(); lossG.backward(); optG.step()\n",
        "        if e % save_every==0:\n",
        "            torch.save(G.state_dict(),f'enh_gen_{e}.pth')\n",
        "            torch.save(D.state_dict(),f'enh_disc_{e}.pth')"
      ],
      "metadata": {
        "id": "uMaix1HdFrAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(ckpt, hr_dir, lr_dir, enhanced=False):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    G = EnhancedGenerator().to(device) if enhanced else Generator().to(device)\n",
        "    G.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "    G.eval()\n",
        "    ds = DIV2KDataset(hr_dir, lr_dir)\n",
        "    ps, ss = [], []\n",
        "    for lr, hr in tqdm(DataLoader(ds,1), desc=\"Eval\"):\n",
        "        lr, hr = lr.to(device), hr.to(device)\n",
        "        with torch.no_grad(): out = G(lr)\n",
        "        out_np = ((out.squeeze().permute(1,2,0).cpu().numpy()*255)).astype(np.uint8)\n",
        "        hr_np  = ((hr.squeeze().permute(1,2,0).cpu().numpy()*255)).astype(np.uint8)\n",
        "        ps.append(psnr(hr_np, out_np, data_range=255))\n",
        "        ss.append(ssim(hr_np, out_np, multichannel=True, data_range=255))\n",
        "    print(f'PSNR: {np.mean(ps):.4f}, SSIM: {np.mean(ss):.4f}')"
      ],
      "metadata": {
        "id": "PGmWIhWvFrDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "train_srgan('DIV2K_train_HR','DIV2K_train_LR',epochs=30)\n",
        "train_enhanced('DIV2K_train_HR','DIV2K_train_LR','gen_200.pth',epochs=30)\n",
        "evaluate('enh_gen_100.pth','DIV2K_train_HR','DIV2K_train_LR',enhanced=True)"
      ],
      "metadata": {
        "id": "fmDi7xTvFrGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90A5MqPYFrJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ONJ3_4LNFrMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2zqSSviFrO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSMdH74LFrSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BJdOQRrFFrU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCV62uyqFrYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1r4vmiSmFrbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AH9xsArrFreD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mem4YEfnFrhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}